# Learning Objectives

- **Instruction-Based Fine-Tuning**
  - Describe how fine-tuning with instructions using prompt datasets can improve performance on one or more tasks.
  - Explain how fine-tuning with instructions using prompt datasets can increase LLM performance on one or more tasks.

- **Catastrophic Forgetting**
  - Define catastrophic forgetting and explain techniques that can be used to overcome it.

- **Parameter-Efficient Fine-Tuning (PEFT)**
  - Define the term Parameter-efficient Fine Tuning (PEFT).
  - Explain how PEFT decreases computational cost and overcomes catastrophic forgetting.

## Course Content

### Fine-Tuning LLMs with Instructions
- Instruction fine-tuning
- Fine-tuning on a single task
- Multi-task instruction fine-tuning
- Scaling instruct models
- Model evaluation
- Benchmarks

### Parameter-Efficient Fine-Tuning (PEFT)
- Parameter-efficient fine-tuning (PEFT)
- PEFT technique 1: LoRA (Low Rank Adaptation)
- PEFT technique 2: Soft Prompts
- Fine-tuning GenAI model for dialogue summarization
