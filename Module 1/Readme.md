# Learning Objectives

- **Model Pre-Training**
  - Discuss the importance and process of model pre-training.
  - Compare the benefits of continued pre-training versus fine-tuning.

- **Key Concepts and Terminology**
  - Define Generative AI and large language models (LLMs).
  - Explain the concept of prompts in the context of LLMs.
  - Describe the transformer architecture that powers LLMs.

- **LLM-Based Generative AI Model Lifecycle**
  - Outline the steps involved in the lifecycle of an LLM-based generative AI model.
  - Identify the factors that influence decision-making at each stage of the model lifecycle.

- **Computational Challenges and Optimization**
  - Discuss the computational challenges encountered during model pre-training.
  - Explore strategies to efficiently reduce memory footprint during training.

- **Scaling Laws for LLMs**
  - Define the term "scaling law."
  - Describe the discovered scaling laws related to training dataset size, compute budget, inference requirements, and other factors.

## Course Content

### Introduction to LLMs and the Generative AI Project Lifecycle
- Overview of Generative AI and LLMs
- LLM use cases and tasks
- Text generation before transformers
- Generating text with transformers
- Prompting and prompt engineering
- Generative configuration
- Generative AI project lifecycle

### LLM Pre-Training and Scaling Laws
- Pre-training large language models
- Computational challenges of training LLMs
- Efficient multi-GPU compute strategies
- Scaling laws and compute-optimal models
- Pre-training for domain adaptation
- Domain-specific training: BloombergGPT
