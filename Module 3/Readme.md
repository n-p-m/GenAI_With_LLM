# Learning Objectives

- **Reinforcement Learning from Human Feedback (RLHF)**
  - Describe how RLHF uses human feedback to improve the performance and alignment of large language models.
  - Explain how data gathered from human labelers is used to train a reward model for RLHF.

- **Advanced Prompting Techniques**
  - Define chain-of-thought prompting and describe how it can be used to improve LLMs' reasoning and planning abilities.

- **Knowledge Cut-Off Challenges**
  - Discuss the challenges that LLMs face with knowledge cut-offs.
  - Explain how information retrieval and augmentation techniques can overcome these challenges.

## Course Content

### Reinforcement Learning from Human Feedback (RLHF)
- Aligning models with human values
- Reinforcement Learning from Human Feedback (RLHF)
- Obtaining feedback from humans for RLHF
- Training a reward model for RLHF
- Fine-tuning with reinforcement learning
- Proximal Policy Optimization (PPO)
- Reward hacking
- KL divergence
- Scaling human feedback
- Fine-tuning FLAN-T5 with reinforcement learning to generate more positive summaries

### LLM-Powered Applications
- Optimizing models for deployment
- Generative AI project lifecycle cheat sheet
- Using LLMs in applications
- Interacting with external applications
- Helping LLMs reason with plan and chain-of-thought
- Program-aided language model (PAL)
- ReAct: Combining reasoning and action
- ReAct: Reasoning and action
- LLM application architectures
